{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:43:31.888487Z",
     "start_time": "2025-06-13T14:43:29.308448Z"
    }
   },
   "source": "from model import *",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T14:43:32.030242Z",
     "start_time": "2025-06-13T14:43:31.895587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    h_dim = 3\n",
    "    max_length = 4\n",
    "    num_heads = 12\n",
    "    d_out = 7\n",
    "\n",
    "    vocab_path = r'C:\\Users\\jaspe\\PycharmProjects\\PythonProject7\\Debugging\\Encoding\\byte_training\\tok.json'\n",
    "    tokenizer = BytePairTokenizer(vocab_file=vocab_path)\n",
    "\n",
    "    vocab_size = max(tokenizer.vocab.values()) + 2\n",
    "    embedding_layer = nn.Embedding(vocab_size, h_dim, padding_idx=0)\n",
    "    pos_encoding_layer = PositionalEncoding(max_length, h_dim)\n",
    "    encoding_layer = Encoding(embedding_layer, pos_encoding_layer)\n",
    "    stacked_attention = MyStackedFunction(\n",
    "        MaskedSelfAttention, num_heads, h_dim, d_out, max_length, dropout=0.1)\n",
    "\n",
    "    print(\"Running batch size validation test...\")\n",
    "    phrases = ['to pimp a butterfly', 'keep it moving forward', 'it is okay to be gay', 'money is not everything']\n",
    "    input_ids = create_token_tensor(phrases, tokenizer, max_length)\n",
    "\n",
    "    encoded_sequences = encoding_layer(input_ids)\n",
    "    context, weights = stacked_attention(encoded_sequences)\n",
    "\n",
    "    assert context.shape == (len(phrases), num_heads, max_length, d_out)\n",
    "    assert weights.shape == (len(phrases), num_heads, max_length, max_length)\n",
    "    print(\"✅ Batch size validation test passed.\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    print(\"\\nRunning attention visualization test...\")\n",
    "    phrase = ['Abraham Lincoln']\n",
    "    input_tensor = create_token_tensor(phrase, tokenizer, max_length)\n",
    "    encoded_single_sequence = encoding_layer(input_tensor)\n",
    "    context_1, weights_1 = stacked_attention(encoded_single_sequence)\n",
    "\n",
    "    token_ids = input_tensor[0].tolist()\n",
    "    string_tokens = [tokenizer.decode([i]) if i != 0 else \"[PAD]\" for i in token_ids]\n",
    "\n",
    "    head_to_show = 2\n",
    "    single_head_weights = weights_1[0, head_to_show, :, :]\n",
    "\n",
    "    visualize_attention(\n",
    "        tokens=string_tokens,\n",
    "        attn_weights=single_head_weights,\n",
    "        head_idx=head_to_show\n",
    "    )\n",
    "    print(\"✅ Attention visualization test finished.\")"
   ],
   "id": "dbfab5148cae183d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaspe\\PycharmProjects\\PythonProject7\\Debugging\\Encoding\\byte_training\\tok.json\n",
      "loaded a model with  4201 vocabulary!\n",
      "target vocab size is:  None !\n",
      "None\n",
      "Running batch size validation test...\n",
      "✅ Batch size validation test passed.\n",
      "--------------------\n",
      "\n",
      "Running attention visualization test...\n",
      "\n",
      "--- Attention Head #2 ---\n",
      "           Abraham    Lincoln   [PAD]     [PAD]   \n",
      "──────────────────────────────────────────────────\n",
      "Abraham      0.01      -inf      -inf      -inf   \n",
      " Lincoln    -0.01     -0.01      -inf      -inf   \n",
      "[PAD]        0.06     -0.07     -0.05      -inf   \n",
      "[PAD]       -0.02      0.06      0.08      0.27   \n",
      "\n",
      "✅ Attention visualization test finished.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T14:43:32.152657Z",
     "start_time": "2025-06-13T14:43:32.143650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "phrase = 'Abraham Lincoln'\n",
    "ids = tokenizer.encode(phrase)\n",
    "\n",
    "emb = encoding_layer(torch.tensor(ids).reshape(-1, 1))\n",
    "attn = stacked_attention(emb)"
   ],
   "id": "d22c379c2ee063ce",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T14:43:32.201341Z",
     "start_time": "2025-06-13T14:43:32.192447Z"
    }
   },
   "cell_type": "code",
   "source": "attn[1].shape",
   "id": "28ee59223d4d4412",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T14:43:34.675746Z",
     "start_time": "2025-06-13T14:43:34.671383Z"
    }
   },
   "cell_type": "code",
   "source": "ids",
   "id": "eb45b2ad18a15e0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53339, 50439]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e0c2e25d8b01dfd2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
